{"cells":[{"cell_type":"markdown","metadata":{"id":"MYpDFySzrk4a"},"source":["# Seminar on Graphs for NLP: Vector representations"]},{"cell_type":"markdown","metadata":{"id":"LlhZKxv4rs2F"},"source":["## Plan for today:\n","\n","#### 0. What a taxonomy is. Taxonomy Enrichment task.\n","#### 1. Graph Neural networks: GCN and GAT\n","#### 2. GATv2\n","#### 3. GraphBERT: Only Attention is Needed for Learning Graph Representations\n","#### 4. GOpenHGNN library"]},{"cell_type":"markdown","metadata":{"id":"6Z0GruYny1kG"},"source":["# 0. Taxonomy\n","\n","A taxonomy is a hierarchical structure of units in terms if class inclusion such that superordinate units in the hierarchy include, or subsume, all items in subordinate units. Taxonomies are typically represented as having tree structures.\n","\n","![](https://www.digital-mr.com/media/cache/51/6f/516f493d37a7b4895f678843b6383e48.png)\n"]},{"cell_type":"markdown","metadata":{"id":"YxGSENmE6IsK"},"source":["Taxonomies can be represented as graphs!\n","\n","Let us download the most popular and well-known taxonomy called WordNet. You may also use the `from nltk.corpus import wordnet as wn`, but keep in mind that you can operate with earlier versions."]},{"cell_type":"code","source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xi-hshM6rL9z","outputId":"19818962-d286-4505-8a6a-6ea0ec6c67f7","executionInfo":{"status":"ok","timestamp":1684499788514,"user_tz":-120,"elapsed":2732161,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.0.1+cu118\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["!pip install tensorboardX"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C91pcHIwsg_v","outputId":"534873de-a934-45a1-b456-e8768e7e7ef4","executionInfo":{"status":"ok","timestamp":1684499792529,"user_tz":-120,"elapsed":4032,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.22.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n","Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.6\n"]}]},{"cell_type":"code","source":["!pip install --upgrade gensim"],"metadata":{"id":"V5VOiYm_2jjR","outputId":"9d21ad72-6aec-4a5c-eb7e-abc39d1e36bb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684499797639,"user_tz":-120,"elapsed":5116,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"]}]},{"cell_type":"code","source":["!gdown --id 1avRebH3BMsolRxmthVFNPoLwyRpAV2tx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtnbZKRXvVhk","outputId":"c474e3e2-e09b-43eb-d2bc-201c54913f05","executionInfo":{"status":"ok","timestamp":1684499803747,"user_tz":-120,"elapsed":6114,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From: https://drive.google.com/uc?id=1avRebH3BMsolRxmthVFNPoLwyRpAV2tx\n","To: /content/wordnet_n_is_directed_1_en_synsets.zip\n","100% 217M/217M [00:04<00:00, 46.1MB/s]\n"]}]},{"cell_type":"code","source":["!unzip wordnet_n_is_directed_1_en_synsets.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ORTr2AWjwIvu","outputId":"6f131bc8-02ad-423e-8655-38dc6eceab8b","executionInfo":{"status":"ok","timestamp":1684499810724,"user_tz":-120,"elapsed":6993,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  wordnet_n_is_directed_1_en_synsets.zip\n","   creating: wordnet_n_is_directed_1_en_synsets/\n","  inflating: wordnet_n_is_directed_1_en_synsets/link  \n","   creating: wordnet_n_is_directed_1_en_synsets/.ipynb_checkpoints/\n","  inflating: wordnet_n_is_directed_1_en_synsets/.ipynb_checkpoints/link-checkpoint  \n","  inflating: wordnet_n_is_directed_1_en_synsets/node  \n"]}]},{"cell_type":"code","source":["!git clone https://github.com/jwzhanggy/Graph-Bert"],"metadata":{"id":"x0vrDJ9cc3qM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('omw-1.4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JTwSPeZcjmZT","outputId":"cd71b336-e4d8-4567-cc25-b6a39f8d92fb","executionInfo":{"status":"ok","timestamp":1684499812873,"user_tz":-120,"elapsed":2159,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["from gensim.models.poincare import PoincareModel\n","import numpy as np\n","import time\n","import os"],"metadata":{"id":"kX-L6-NLeKJ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import wordnet as wn"],"metadata":{"id":"Ap6gnMUfy146"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hiwh7Xhcy9hm","outputId":"2daa1223-47c1-4a18-dc0c-a79b39c0b150","executionInfo":{"status":"ok","timestamp":1684499813321,"user_tz":-120,"elapsed":7,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["wn.synset(\"guy.n.01\").lemmas()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"40OqktlGy5Wd","outputId":"f82d63c2-8ad6-4683-a5bf-4b4978998778","executionInfo":{"status":"ok","timestamp":1684499816132,"user_tz":-120,"elapsed":2815,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Lemma('guy.n.01.guy'),\n"," Lemma('guy.n.01.cat'),\n"," Lemma('guy.n.01.hombre'),\n"," Lemma('guy.n.01.bozo')]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["path = f\"wordnet_n_is_directed_1_en_synsets/\"\n","\n","link_path = os.path.join(path, \"link\")\n","node_path = os.path.join(path, \"node\")"],"metadata":{"id":"XeGdLaEqekbb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["id2synset = {}\n","fasttext_dict = {}\n","\n","with open(node_path) as f:\n","    for line in f:\n","        line_split = line.split(\"\\t\")\n","        id2synset[line_split[0].strip()] = line_split[-1].strip()\n","        fasttext_dict[line_split[-1].strip()] = np.array([float(num) for num in line_split[1:-1]])"],"metadata":{"id":"C26HaOfGfDkR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["link_pairs = set()\n","with open(link_path) as f:\n","    for line in f:\n","        line_split = line.split(\"\\t\")\n","        link_pairs.add((id2synset[line_split[0].strip()], id2synset[line_split[-1].strip()]))"],"metadata":{"id":"i7hmkbEXfFIG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Arh9U60uDH8"},"source":["# 4. Graph Neural Networks"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"46UD5px_rcNb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch_geometric.nn as pyg_nn\n","import torch_geometric.utils as pyg_utils"],"metadata":{"id":"B1qcGlrbre_y"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eM2IXSZMuIq8"},"outputs":[],"source":["import time\n","from datetime import datetime\n","\n","import networkx as nx\n","import numpy as np\n","import torch\n","import torch.optim as optim\n","\n","from torch_geometric.datasets import TUDataset\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.data import DataLoader\n","from torch_geometric.utils import train_test_split_edges\n","import torch_geometric.transforms as T\n","from torch_geometric.data import Data\n","\n","from tensorboardX import SummaryWriter\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"markdown","metadata":{"id":"KPMU0mL6IJEA"},"source":["## Data preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2NAxJKWLmkae"},"outputs":[],"source":["from gensim.models.keyedvectors import KeyedVectors\n","\n","fasttext = KeyedVectors(vector_size=300)\n","fasttext.add_vectors(list(fasttext_dict.keys()), list(fasttext_dict.values()))"]},{"cell_type":"code","source":["import networkx as nx"],"metadata":{"id":"JL1yQgaD4mio"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["G = nx.DiGraph()\n","\n","for pair in link_pairs:\n","    G.add_edge(*pair)"],"metadata":{"id":"WkLOHE1t4qJo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cgC4chwuIJEC"},"outputs":[],"source":["def create_edge_list(G):\n","    starts = []\n","    ends = []\n","    for left, right in G.edges:\n","        if left in fasttext.key_to_index and right in fasttext.key_to_index:\n","            starts.append(fasttext.key_to_index[left])\n","            ends.append(fasttext.key_to_index[right])\n","    return torch.tensor([starts, ends], dtype=torch.long)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ILgb5IF4a2A9"},"outputs":[],"source":["index_to_key = dict(map(reversed, fasttext.key_to_index.items()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_iX7WH2nCSv"},"outputs":[],"source":["edge_index = create_edge_list(G)"]},{"cell_type":"code","source":["x = torch.tensor([fasttext[index_to_key[int(i)]] for i in index_to_key], dtype=torch.float)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ZJdPogQy1kG","outputId":"7346a26f-df16-43ce-abdd-6baf3ad68823","executionInfo":{"status":"ok","timestamp":1684499841047,"user_tz":-120,"elapsed":2850,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-23-ceafeacad9cc>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n","  x = torch.tensor([fasttext[index_to_key[int(i)]] for i in index_to_key], dtype=torch.float)\n"]}]},{"cell_type":"code","source":["data = Data(x=x, edge_index=edge_index)\n","#data = train_test_split_edges(data)"],"metadata":{"id":"WC3VgJ0FyiiH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch_geometric.transforms import RandomLinkSplit"],"metadata":{"id":"TCVO4jfK1LFI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform = RandomLinkSplit(is_undirected=True, split_labels=True)\n","train_data, val_data, test_data = transform(data)"],"metadata":{"id":"lPA8jPvZ1IIn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uCZE_0e0IJED"},"source":["### GCN and GAT Encoder\n","\n","The following code snippet describes the Encoder module with GCN or GAT networks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-naFqNRumvk"},"outputs":[],"source":["class Encoder(torch.nn.Module):\n","    def __init__(self, in_channels, out_channels, mode=\"gcn\"):\n","        super(Encoder, self).__init__()\n","        if mode == \"gcn\":\n","            self.conv1 = pyg_nn.GCNConv(in_channels, 2 * out_channels, cached=True)\n","            self.conv2 = pyg_nn.GCNConv(2 * out_channels, out_channels, cached=True)\n","        elif mode == 'gat':\n","            self.conv1 = pyg_nn.GATConv(in_channels, 2 * out_channels)\n","            self.conv2 = pyg_nn.GATConv(2 * out_channels, out_channels)\n","        else:\n","            raise Exception(\"Encoder mode is not recognized, try gcn/gat\")\n","\n","    def forward(self, x, edge_index):\n","        x = F.relu(self.conv1(x, edge_index))\n","        return self.conv2(x, edge_index)\n","\n","def train(epoch):\n","    model.train()\n","    optimizer.zero_grad()\n","    z = model.encode(x, train_pos_edge_index)\n","    loss = model.recon_loss(z, train_pos_edge_index)\n","    loss.backward()\n","    optimizer.step()\n","    writer.add_scalar(\"loss\", loss.item(), epoch)\n","    return loss.item()\n","\n","def test(pos_edge_index, neg_edge_index):\n","    model.eval()\n","    with torch.no_grad():\n","        z = model.encode(x, train_pos_edge_index)\n","    return model.test(z, pos_edge_index, neg_edge_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eplyGdMwvF57","outputId":"79131632-584e-428b-eea5-b958302b8a2d","executionInfo":{"status":"ok","timestamp":1684499841416,"user_tz":-120,"elapsed":9,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA availability: True\n"]}],"source":["writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","\n","channels = 64\n","dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('CUDA availability:', torch.cuda.is_available())"]},{"cell_type":"markdown","metadata":{"id":"UHEHYEbcXCFG"},"source":["## Variational Graph Auto-Encoders\n","\n","https://arxiv.org/pdf/1611.07308.pdf\n","\n","The pipeline is working as follows: first, we train a graph autoencoder with GCN or GAT under the hoot. During the evaluation phase, the latent representations of the autoencoder are actually the embeddings we are looking for."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqZGTsQ7vRQM","outputId":"5b9b671c-3fbc-4389-ef5c-51d9ec00400e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684499881605,"user_tz":-120,"elapsed":40195,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 010, AUC: 0.8353, AP: 0.8220, Loss: 0.9354\n","Epoch: 020, AUC: 0.8730, AP: 0.8669, Loss: 0.8543\n","Epoch: 030, AUC: 0.8932, AP: 0.8880, Loss: 0.8268\n","Epoch: 040, AUC: 0.8973, AP: 0.8962, Loss: 0.8096\n","Epoch: 050, AUC: 0.9029, AP: 0.9037, Loss: 0.7987\n","Epoch: 060, AUC: 0.9050, AP: 0.9073, Loss: 0.7874\n","Epoch: 070, AUC: 0.9077, AP: 0.9111, Loss: 0.7850\n","Epoch: 080, AUC: 0.9117, AP: 0.9145, Loss: 0.7795\n","Epoch: 090, AUC: 0.9107, AP: 0.9150, Loss: 0.7801\n","Epoch: 100, AUC: 0.9115, AP: 0.9160, Loss: 0.7733\n","Epoch: 110, AUC: 0.9115, AP: 0.9169, Loss: 0.7753\n","Epoch: 120, AUC: 0.9122, AP: 0.9175, Loss: 0.7691\n","Epoch: 130, AUC: 0.9133, AP: 0.9187, Loss: 0.7701\n","Epoch: 140, AUC: 0.9138, AP: 0.9198, Loss: 0.7685\n","Epoch: 150, AUC: 0.9141, AP: 0.9200, Loss: 0.7662\n","Epoch: 160, AUC: 0.9143, AP: 0.9207, Loss: 0.7653\n","Epoch: 170, AUC: 0.9146, AP: 0.9212, Loss: 0.7652\n","Epoch: 180, AUC: 0.9140, AP: 0.9206, Loss: 0.7645\n","Epoch: 190, AUC: 0.9153, AP: 0.9222, Loss: 0.7613\n","Epoch: 200, AUC: 0.9150, AP: 0.9219, Loss: 0.7588\n","Epoch: 210, AUC: 0.9151, AP: 0.9225, Loss: 0.7591\n","Epoch: 220, AUC: 0.9157, AP: 0.9229, Loss: 0.7590\n","Epoch: 230, AUC: 0.9162, AP: 0.9234, Loss: 0.7593\n","Epoch: 240, AUC: 0.9160, AP: 0.9234, Loss: 0.7573\n","Epoch: 250, AUC: 0.9157, AP: 0.9232, Loss: 0.7581\n","Epoch: 260, AUC: 0.9156, AP: 0.9232, Loss: 0.7594\n","Epoch: 270, AUC: 0.9151, AP: 0.9227, Loss: 0.7559\n","Epoch: 280, AUC: 0.9149, AP: 0.9230, Loss: 0.7564\n","Epoch: 290, AUC: 0.9151, AP: 0.9233, Loss: 0.7559\n","Epoch: 300, AUC: 0.9148, AP: 0.9231, Loss: 0.7555\n","Epoch: 310, AUC: 0.9149, AP: 0.9227, Loss: 0.7517\n","Epoch: 320, AUC: 0.9138, AP: 0.9227, Loss: 0.7536\n","Epoch: 330, AUC: 0.9138, AP: 0.9227, Loss: 0.7538\n","Epoch: 340, AUC: 0.9137, AP: 0.9224, Loss: 0.7511\n","Epoch: 350, AUC: 0.9126, AP: 0.9214, Loss: 0.7524\n","Epoch: 360, AUC: 0.9124, AP: 0.9214, Loss: 0.7479\n","Epoch: 370, AUC: 0.9122, AP: 0.9212, Loss: 0.7531\n","Epoch: 380, AUC: 0.9128, AP: 0.9220, Loss: 0.7503\n","Epoch: 390, AUC: 0.9129, AP: 0.9224, Loss: 0.7485\n","Epoch: 400, AUC: 0.9126, AP: 0.9220, Loss: 0.7499\n"]}],"source":["model = pyg_nn.GAE(Encoder(300, channels, 'gcn')).to(dev)\n","x, train_pos_edge_index = train_data.x.to(dev), train_data.pos_edge_label_index.to(dev)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","for epoch in range(1, 401):\n","    loss = train(epoch)\n","    auc, ap = test(test_data.pos_edge_label_index, test_data.neg_edge_label_index)\n","    writer.add_scalar(\"AUC\", auc, epoch)\n","    writer.add_scalar(\"AP\", ap, epoch)\n","    if epoch % 10 == 0:\n","        print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}, Loss: {:.4f}'.format(epoch, auc, ap, loss))"]},{"cell_type":"markdown","metadata":{"id":"3__lKOCOXNKj"},"source":["#### Examples\n","\n","Let us see the nearest neighbours for the unseen words from the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mWcc2HlvVLf"},"outputs":[],"source":["model.eval()\n","new_x = torch.tensor([fasttext[index_to_key[i]] for i in index_to_key], dtype=torch.float).to(dev)\n","z = model.encode(new_x, train_pos_edge_index)"]},{"cell_type":"code","source":["id2syns = {}\n","syns2id = {}\n","with open('wordnet_n_is_directed_1_en_synsets/node') as f:\n","    for line in f:\n","        id2syns[line.split()[0]] = line.split()[-1]\n","        syns2id[line.split()[-1]] = line.split()[0]"],"metadata":{"id":"HoDN-uTq8bem"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["par2orph = {}\n","orph2par = {}\n","with open('wordnet_n_is_directed_1_en_synsets/link') as f:\n","    for line in f:\n","        par_id = line.split()[0]\n","        child_id = line.split()[-1]\n","        \n","        if \"ORPHAN_\" in id2syns[child_id]:\n","            par2orph[id2syns[par_id]] = id2syns[child_id]\n","            orph2par[id2syns[child_id]] = id2syns[par_id]"],"metadata":{"id":"DGWVX-WL74KV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["c = 0\n","for word in fasttext.key_to_index:\n","    if \".n.\" not in word:\n","        cur_index = fasttext.key_to_index[word]\n","        tensor_ = torch.tensor([[cur_index]*(len(G.nodes)), [i for i in range(0, len(G.nodes))]])\n","        results = model.decode(z, tensor_)\n","        top10 = list(reversed(sorted([(index_to_key[i], round(float(score.cpu().detach().float()), 4)) for i, score in enumerate(results)], key=lambda x: x[1])))[:10]       \n","        print(orph2par[word], \":\", top10)\n","        print(\"=\"*10)\n","        c += 1\n","        if c == 15:\n","            break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c4jW1K0d3Fq1","outputId":"28c0f319-d9c9-4c3a-a726-28d785754225","executionInfo":{"status":"ok","timestamp":1684499921528,"user_tz":-120,"elapsed":28787,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["course.n.04 : [('act.n.05', 0.997), ('act.n.03', 0.997), ('action.n.05', 0.9895), ('pump_action.n.01', 0.9862), ('course.n.04', 0.984), ('movement.n.10', 0.9837), ('ORPHAN_100000000', 0.9815), ('job_action.n.01', 0.9805), ('police_action.n.01', 0.9795), ('scheme.n.01', 0.9763)]\n","==========\n","recovery.n.03 : [('reclamation.n.02', 0.9554), ('redemption.n.01', 0.9545), ('bestowal.n.01', 0.9536), ('salvation.n.04', 0.9499), ('search_and_rescue_mission.n.01', 0.9481), ('contribution.n.03', 0.9468), ('lifesaving.n.01', 0.9464), ('accordance.n.02', 0.9424), ('salvage.n.02', 0.9419), ('salvage.n.03', 0.9419)]\n","==========\n","disappearance.n.01 : [('shading.n.01', 0.9468), ('flit.n.02', 0.9313), ('move.n.05', 0.9283), ('blaze.n.05', 0.9282), ('spot.n.05', 0.9277), ('gradient.n.01', 0.9248), ('difference.n.04', 0.9235), ('rewording.n.01', 0.9231), ('foray.n.02', 0.923), ('turning.n.03', 0.9229)]\n","==========\n","hit.n.03 : [('base_hit.n.01', 0.9958), ('hit.n.05', 0.9927), ('hit.n.01', 0.9927), ('slap.n.01', 0.9862), ('knock.n.03', 0.9846), ('pounding.n.01', 0.9798), ('concussion.n.02', 0.9786), ('zap.n.01', 0.9785), ('sideswipe.n.01', 0.9783), ('ORPHAN_100000003', 0.9783)]\n","==========\n","breach.n.01 : [('act.n.05', 0.9998), ('act.n.03', 0.9998), ('breach.n.01', 0.9991), ('breach.n.02', 0.9979), ('ORPHAN_100000004', 0.9975), ('disappointment.n.02', 0.9963), ('copout.n.01', 0.9954), ('damage.n.01', 0.9949), ('failure.n.05', 0.994), ('special_act.n.01', 0.9928)]\n","==========\n","buying.n.01 : [('buying.n.01', 0.997), ('shopping.n.01', 0.9941), ('ORPHAN_100000005', 0.9937), ('redemption.n.03', 0.9905), ('purchase.n.01', 0.9837), ('obtainment.n.01', 0.9835), ('acquisition.n.01', 0.9818), ('catching.n.03', 0.981), ('capture.n.01', 0.9778), ('reception.n.04', 0.9763)]\n","==========\n","restitution.n.03 : [('financial_gain.n.01', 0.9426), ('fulfillment.n.02', 0.9306), ('relief.n.03', 0.9278), ('punitive_damages.n.01', 0.9272), ('non-cash_expense.n.01', 0.9272), ('entail.n.01', 0.9252), ('entail.n.02', 0.9252), ('realization.n.06', 0.9227), ('actual_damages.n.01', 0.9215), ('nominal_damages.n.01', 0.921)]\n","==========\n","abandonment.n.03 : [('abandonment.n.03', 0.9844), ('sewage_disposal.n.01', 0.976), ('appointment.n.06', 0.9759), ('giving.n.03', 0.9758), ('lending.n.01', 0.9741), ('purification.n.04', 0.9677), ('mine_disposal.n.01', 0.9677), ('accession.n.01', 0.9668), ('ORPHAN_100000007', 0.9665), ('prefixation.n.01', 0.9631)]\n","==========\n","mine_disposal.n.01 : [('rescue_equipment.n.01', 0.8983), ('naval_equipment.n.01', 0.8897), ('materiel.n.01', 0.8883), ('strip_mining.n.01', 0.8858), ('teaching_aid.n.01', 0.8827), ('electronics_intelligence.n.01', 0.8811), ('robotics_equipment.n.01', 0.8809), ('porterage.n.01', 0.8768), ('radiotherapy_equipment.n.01', 0.8764), ('gear.n.04', 0.8753)]\n","==========\n","expiation.n.02 : [('logical_proof.n.01', 0.9956), ('act.n.05', 0.9954), ('act.n.03', 0.9954), ('mathematical_proof.n.01', 0.9954), ('demonstration.n.04', 0.993), ('ORPHAN_100000009', 0.9911), ('punitive_damages.n.01', 0.9907), ('actual_damages.n.01', 0.9901), ('derogation.n.02', 0.9895), ('proof.n.02', 0.9893)]\n","==========\n","rendition.n.04 : [('rewording.n.01', 0.97), ('planning.n.02', 0.97), ('difference.n.04', 0.9655), ('rendering.n.06', 0.9623), ('gradient.n.01', 0.9612), ('broad_interpretation.n.01', 0.9611), ('rendition.n.01', 0.9602), ('ORPHAN_100000010', 0.9601), ('rendering.n.07', 0.9597), ('prior.n.01', 0.9568)]\n","==========\n","depression.n.10 : [('reset_button.n.01', 0.9272), ('nudge.n.01', 0.922), ('depression.n.10', 0.9206), ('push_button.n.01', 0.9003), ('concern.n.02', 0.8992), ('duress.n.01', 0.8927), ('bundling.n.03', 0.8912), ('boost.n.03', 0.8909), ('angst.n.01', 0.8906), ('scruple.n.02', 0.8904)]\n","==========\n","blink.n.01 : [('eye.n.05', 0.9725), ('wet_dream.n.01', 0.9548), ('nightmare.n.02', 0.9546), ('malayan_tapir.n.01', 0.9522), ('tapir.n.01', 0.9367), ('nay.n.01', 0.9352), ('dream.n.01', 0.9346), ('snake.n.01', 0.9232), ('southwestern_toad.n.01', 0.9181), ('decapod.n.02', 0.9175)]\n","==========\n","shooting.n.01 : [('shot.n.12', 0.9981), ('set_shot.n.01', 0.9939), ('pivot_shot.n.01', 0.9935), ('cheap_shot.n.02', 0.9923), ('scoop_shot.n.01', 0.9921), ('dunk.n.01', 0.992), ('foul_shot.n.01', 0.9919), ('jumper.n.08', 0.9909), ('lay-up.n.01', 0.9876), ('shoot.n.02', 0.9873)]\n","==========\n","hit.n.02 : [('base_hit.n.01', 0.9278), ('horn_fly.n.01', 0.9099), ('housefly.n.01', 0.9089), ('tsetse_fly.n.01', 0.9077), ('hit.n.05', 0.9027), ('hit.n.01', 0.9027), ('fly.n.01', 0.8932), ('flip.n.04', 0.8399), ('fling.n.03', 0.8333), ('golf_ball.n.01', 0.8303)]\n","==========\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZgbmlZqBIJEN"},"source":["## GraphBERT\n","\n","https://github.com/jwzhanggy/Graph-Bert\n","\n","Yet another model for embedding generation is GraphBert. Instead of feeding large input graph, we train GRAPH-BERT with sampled subgraphs within their local contexts. The input vector embeddings to be fed to the graphtransformer model actually cover four parts: (1) raw feature vector embedding, (2) Weisfeiler-Lehman absolute role embedding, (3) intimacy based relative positional embedding, and (4) hop based relative distance embedding, respectively.\n","\n","GRAPH-BERT is trained with the node attribute reconstruction and structure recovery tasks."]},{"cell_type":"markdown","metadata":{"id":"zo0z3trCa2BC"},"source":["![](https://github.com/jwzhanggy/Graph-Bert/raw/master/result/screenshot/model.png)"]},{"cell_type":"markdown","metadata":{"id":"1fOqFVHLa2BD"},"source":["## Subgraph Sampling"]},{"cell_type":"markdown","metadata":{"id":"5htpR5J7a2BD"},"source":["![](https://i.ibb.co/5cbjJZ6/photo-2021-12-07-16-41-32.jpg)"]},{"cell_type":"markdown","metadata":{"id":"XDsdGrffa2BD"},"source":["## Positional embeddings"]},{"cell_type":"markdown","metadata":{"id":"tOny-pLQa2BE"},"source":["### Weisfeiler-Lehman Absolute Role Embedding\n","\n","![](https://i.ibb.co/bgT7gqb/wl.png)\n","\n","### Intimacy based Relative Positional Embedding\n","\n","![](https://i.ibb.co/34FvCf0/photo-2021-12-07-16-52-30.jpg)\n","\n","### Hop based Relative Distance Embedding\n","![](https://i.ibb.co/tCzRcfK/hops-drawio.png)"]},{"cell_type":"markdown","metadata":{"id":"AomlgEcXa2BE"},"source":["Actually, you are simply expected to run two scripts: `script_1_preprocess.py` and `script_2_pre_train.py`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9DXA9H5la2BF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5b6a038f-1f76-44f7-cc54-bfe437cc756d","executionInfo":{"status":"ok","timestamp":1684499921979,"user_tz":-120,"elapsed":466,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Graph-Bert'...\n","remote: Enumerating objects: 450, done.\u001b[K\n","remote: Counting objects: 100% (136/136), done.\u001b[K\n","remote: Compressing objects: 100% (58/58), done.\u001b[K\n","remote: Total 450 (delta 106), reused 79 (delta 78), pack-reused 314\u001b[K\n","Receiving objects: 100% (450/450), 2.23 MiB | 15.41 MiB/s, done.\n","Resolving deltas: 100% (232/232), done.\n"]}],"source":["!git clone https://github.com/jwzhanggy/Graph-Bert.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ueo45TqXa2BF","outputId":"502f8167-d9fe-474a-aa19-d38d37178cdf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684500031810,"user_tz":-120,"elapsed":109837,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Graph-Bert\n","************ Start ************\n","WL, dataset: cora\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 1\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 2\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 3\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 4\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 5\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 6\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 7\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 8\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 9\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","Subgraph Batching, dataset: cora, k: 10\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 1\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 2\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 3\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 4\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 5\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 6\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 7\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 8\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 9\n","Loading cora dataset...\n","************ Finish ************\n","************ Start ************\n","HopDistance, dataset: cora, k: 10\n","Loading cora dataset...\n","************ Finish ************\n"]}],"source":["%cd Graph-Bert\n","!python3 script_1_preprocess.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DkH0wplPa2BG","outputId":"1b7d14df-31e8-4fbf-e929-2a045e7e06a8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684500033691,"user_tz":-120,"elapsed":1889,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/content/Graph-Bert/script_2_pre_train.py\", line 5, in <module>\n","    from code.MethodBertComp import GraphBertConfig\n","  File \"/content/Graph-Bert/code/MethodBertComp.py\", line 11, in <module>\n","    from transformers.modeling_bert import BertPredictionHeadTransform, BertAttention, BertIntermediate, BertOutput\n","ModuleNotFoundError: No module named 'transformers'\n"]}],"source":["!python3 script_2_pre_train.py"]},{"cell_type":"markdown","metadata":{"id":"HYtahIlwa2BG"},"source":["After the model has been trained, we predict embeddings for the new (unseen words) and their nearest neighbours."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vgurRJ-Ca2BH","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1684500034238,"user_tz":-120,"elapsed":551,"user":{"displayName":"Irina Nikishina","userId":"06548341279621459266"}},"outputId":"5b448749-33c5-44cf-eb8a-7f413133b700"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-85005f0cd4d1>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/nikishina/Graph-Bert/code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/nikishina/Graph-Bert/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mDatasetLoader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mMethodBertComp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphBertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mMethodGraphBertGraphRecovery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMethodGraphBertGraphRecovery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DatasetLoader'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import os\n","import sys\n","\n","import numpy as np\n","from nltk.corpus import wordnet as wn\n","\n","sys.path.append(\"/home/nikishina/Graph-Bert/code\")\n","sys.path.append(\"/home/nikishina/Graph-Bert/\")\n","from DatasetLoader import DatasetLoader\n","from MethodBertComp import GraphBertConfig\n","from MethodGraphBertGraphRecovery import MethodGraphBertGraphRecovery\n","from MethodGraphBertNodeConstruct import MethodGraphBertNodeConstruct\n","from itertools import combinations\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n","\n","\n","def load_data(dataset_path, k, device):\n","    data_obj = DatasetLoader()\n","    data_obj.dataset_source_folder_path = '/home/nikishina/Graph-Bert/data/' + dataset_path + '/'\n","    data_obj.dataset_name = dataset_path\n","    data_obj.k = k\n","    data_obj.device = device\n","    data_obj.load_all_tag = True\n","    return data_obj.load()\n","\n","\n","def get_query_embedding(word, final_embeddings, index_id_map):\n","    offset, definition = wn.synset(word).offset(), wn.synset(word).definition()\n","    index_of_synset = None\n","\n","    for i, j in index_id_map.items():\n","        if j == offset:\n","            index_of_synset = i\n","            break\n","\n","    query_embedding = final_embeddings[index_of_synset]\n","    return query_embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbIUXhyia2BH"},"outputs":[],"source":["class GraphBERTEmbeddingsSaver:\n","    def __init__(self, model_name, model, x_size=300, device='cpu', max_index=132, intermediate_size=32,\n","                 num_attention_heads=2, num_hidden_layers=2, y_size=0, residual_type='graph_raw', k=5, nfeature=300):\n","\n","        pretrained_path = './result/PreTrained_GraphBert/' + model_name\n","        bert_config = GraphBertConfig(residual_type=residual_type, k=k, x_size=x_size, y_size=y_size,\n","                                      hidden_size=intermediate_size, intermediate_size=intermediate_size,\n","                                      num_attention_heads=num_attention_heads, num_hidden_layers=num_hidden_layers,\n","                                      max_wl_role_index=max_index, max_hop_dis_index=max_index,\n","                                      max_inti_pos_index=max_index)\n","\n","        self.model = model(bert_config, pretrained_path, device=device)\n","        self.model.eval()\n","        self.nfeature = nfeature\n","\n","    def compute_and_save_embeddings(self, data, test_synsets, index_id_map, id2label, result_dir):\n","        final_embeddings = self.compute_embeddings(data, index_id_map, id2label)\n","        self.save_embeddings(test_synsets, final_embeddings, result_dir)\n","\n","    def compute_embeddings(self, data, index_id_map, id2label):\n","        final_embeddings = np.zeros(shape=(len(index_id_map), self.nfeature), dtype=np.float32)\n","\n","        for _index, raw_f, wl, init, hop in zip(index_id_map, *data):\n","            final_embeddings[_index, :] = np.array(\n","                self.model(raw_f.unsqueeze(0), wl.unsqueeze(0), init.unsqueeze(0), hop.unsqueeze(0))[0]\n","                    .cpu().detach())\n","        return self.get_embeddings_dict(final_embeddings, index_id_map, id2label)\n","\n","    @staticmethod\n","    def get_embeddings_dict(embeddings, index2id_map, id2label):\n","        return {id2label[index]: embeddings[_id] for _id, index in index2id_map.items()}\n","\n","    def save_embeddings(self, test_synsets, embeddings, result_dir):\n","        with open(os.path.join(result_dir, f\"{self.model.__class__.__name__}_model_train_embeddings.txt\"), 'w') as w1:\n","            with open(os.path.join(result_dir, f\"{self.model.__class__.__name__}_model_test_embeddings.txt\"),\n","                      'w') as w2:\n","                for synset_name, embedding in embeddings.items():\n","                    if synset_name in test_synsets:\n","                        text_embedding = \" \".join([str(e) for e in embedding])\n","                        w2.write(f\"{synset_name} {text_embedding}\\n\")\n","                    else:\n","                        text_embedding = \" \".join([str(e) for e in embedding])\n","                        w1.write(f\"{synset_name} {text_embedding}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"frvTximja2BI"},"outputs":[],"source":["loaded_data = load_data('wordnet_n_is_directed_1_en_synsets_2.0', 5, 'cpu')\n","dataset = (loaded_data['raw_embeddings'], loaded_data['wl_embedding'], loaded_data['hop_embeddings'],\n","           loaded_data['int_embeddings'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uj1GzbATa2BJ"},"outputs":[],"source":["index_id_map = loaded_data['index_id_map']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WetYSlefa2BJ"},"outputs":[],"source":["idx_features_labels = np.genfromtxt(\"{}/node\".format('/home/nikishina/Graph-Bert/data/wordnet_n_is_directed_1_en_synsets_2.0/'), dtype=np.dtype(str))\n","id2label = {int(i): j for i, j in zip(idx_features_labels[:, 0], idx_features_labels[:, -1])}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ahwRGzqZa2BJ"},"outputs":[],"source":["saver = GraphBERTEmbeddingsSaver('wordnet_n_is_directed_1_en_synsets_2.0/node_reconstruct_model', MethodGraphBertNodeConstruct)\n","saver.compute_and_save_embeddings(dataset, new_words, index_id_map, id2label, \"../\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MB-tbeNba2BK"},"outputs":[],"source":["saver = GraphBERTEmbeddingsSaver('wordnet_n_is_directed_1_en_synsets_2.0/node_graph_reconstruct_model', MethodGraphBertGraphRecovery)\n","saver.compute_and_save_embeddings(dataset, new_words, index_id_map, id2label, \"../\")"]},{"cell_type":"markdown","metadata":{"id":"tE-7RzJ1a2BK"},"source":["## View and evaluate results"]},{"cell_type":"code","source":["!gdown 1IAfd9tRgtVtdosM5vuDdxh-VSBFp3mzI\n","!gdown 1LItbxEcchOfU4TrlLBZjQweC8jpQ3b3Q\n","!gdown 1VLLLyu9YyLX3uCojiTm_VLtgK2gKCCfW\n","!gdown 1h5sSbFeCJbouH96fKIZDF2xugNiKf3La"],"metadata":{"id":"zvWT_UAaSW7m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.models import KeyedVectors"],"metadata":{"id":"uLCgr2ibUZL9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bW9Z8hLca2BL"},"outputs":[],"source":["graphBertNode_train = KeyedVectors.load_word2vec_format(\"MethodGraphBertNodeConstruct_model_train_embeddings_.txt\")\n","graphBertNode_test = KeyedVectors.load_word2vec_format(\"MethodGraphBertNodeConstruct_model_test_embeddings.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zdnqyA7ia2BL"},"outputs":[],"source":["graphBertNode_train.similar_by_word(\"chocolate_milk.n.01\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EVHNs_-Za2BM"},"outputs":[],"source":["graphbert_node_predicts = {}\n","\n","for word in fasttext.key_to_index:\n","    if \".n.\" not in word:\n","        graphbert_node_predicts[word] = graphBertNode_train.similar_by_vector(graphBertNode_test[word])"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1XJSURKU7_qVhLnRLuOTLqBPSxxz75Ot1","timestamp":1686488280566},{"file_id":"1YcIltjIofG6JHwg1AwDevcf8uXxc0fAM","timestamp":1684506905239}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}